{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6684b36d-7750-414c-9713-92dd5ade01ac",
   "metadata": {},
   "source": [
    "# 3D-SOCS 3D tracking pipeline demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674d880-8d08-4d37-9495-4732d0fcb10d",
   "metadata": {},
   "source": [
    "This jupyter notebook runs through a few important functions to run the 3D tracking pipeline. For the purpose of this notebook, we will use the official sample dataset, which can be downloaded [here](https://doi.org/10.17617/3.ZQMOJ3). This notebook only goes through key steps with the tracking pipeline, assuming you have already collected frame-synchronized videos from 3D-SOCS. For more infomration on hardware and software requirements for setting up 3D-SOCS, please refer to the raspi instructions in the [github repository](https://github.com/alexhang212/3D-SOCS/tree/main/3DSOCS_raspi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c255d29",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1bf51c",
   "metadata": {},
   "source": [
    "The first step is multi-view calibration. You will need to collect calibration sequences of a charuco board, moving through the scene. We first need to import required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef25e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from cv2 import aruco\n",
    "\n",
    "# Add the current directory to the system path\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "from Calibration import ExtrinsicCalibration\n",
    "from Calibration import IntrinsicCalibration\n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1062da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## you need to define the path to the videos and the path to a data folder where calibration data will be stored\n",
    "\n",
    "InputDir = \"../SampleDataset/Calibration/videos\"\n",
    "DataDir = \"../SampleDataset/Calibration/data\"\n",
    "CamNames = [\"mill1\",\"mill2\",\"mill3\",\"mill4\",\"mill5\",\"mill6\"]\n",
    "\n",
    "##this creates the data directory if it does not exist\n",
    "if not os.path.exists(DataDir):\n",
    "    os.mkdir(DataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7554319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of calibration board parameters used in the experiment\n",
    "\n",
    "CalibBoardDict = {\n",
    "    \"widthNum\": 5,  # Number of squares in width\n",
    "    \"lengthNum\" : 8,  # Number of squares in length\n",
    "    \"squareLen\" : 24,  # Length of a square in mm\n",
    "    \"arucoLen\" : 19,  # Length of an ArUco marker in mm\n",
    "    \"ArucoDict\" : aruco.getPredefinedDictionary(aruco.DICT_4X4_50)  # Predefined ArUco dictionary\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8ac882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3489/3489 [00:15<00:00, 220.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimization algorithm to improve calibration\n",
      "Images Removed: 1\n",
      "New Error is 0.892753\n",
      "Final Error: 0.892753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3489/3489 [00:05<00:00, 583.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 0.561220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3489/3489 [00:07<00:00, 490.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 0.407633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3489/3489 [00:04<00:00, 740.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 0.643240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3489/3489 [00:07<00:00, 462.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 0.457875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3489/3489 [00:06<00:00, 575.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 0.441298\n"
     ]
    }
   ],
   "source": [
    "# Then we can first do intrinsics calibration\n",
    "VideoPaths = sorted(glob(os.path.join(InputDir, \"*.mp4\")))\n",
    "\n",
    "# Intrinsic calibration for each camera\n",
    "for x in range(len(CamNames)):\n",
    "    InputVid = VideoPaths[x]\n",
    "    # Perform intrinsic calibration\n",
    "    retCal, cameraMatrix, distCoeffs, rvecs, tvecs, pVErr, NewCamMat, roi = IntrinsicCalibration.calibrate_auto(\n",
    "        InputVid, CalibBoardDict, Freq=10, debug=False\n",
    "    )\n",
    "    # Save intrinsic calibration results\n",
    "    IntSavePath = os.path.join(DataDir, \"%s_Intrinsic.p\" % CamNames[x])\n",
    "    pickle.dump((retCal, cameraMatrix, distCoeffs, rvecs, tvecs, pVErr, NewCamMat, roi), open(IntSavePath, \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a0a1c",
   "metadata": {},
   "source": [
    "This will output intrinsic calibration paramters, in terms of the 3x3 intrinsic matrix and the 5x1 distortion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d496b9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Matrix\n",
      "[[1.42374704e+03 0.00000000e+00 6.47493853e+02]\n",
      " [0.00000000e+00 1.42190035e+03 3.70208574e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
      "Distortion Coefficients\n",
      "[[-9.91312998e-03  4.15260744e-01  2.61386448e-03 -2.33446376e-04\n",
      "  -1.24539145e+00]]\n"
     ]
    }
   ],
   "source": [
    "##Load example from camera 1:\n",
    "pickle.load(open(os.path.join(DataDir, \"mill1_Intrinsic.p\"), \"rb\"))\n",
    "\n",
    "##Intrsinsic camera matrix\n",
    "print(\"Camera Matrix\")\n",
    "print(cameraMatrix)\n",
    "\n",
    "##distortion coefficients\n",
    "print(\"Distortion Coefficients\")\n",
    "print(distCoeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ce0611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3489/3489 [02:46<00:00, 20.96it/s]\n",
      "100%|██████████| 3489/3489 [01:06<00:00, 52.84it/s]\n",
      "100%|██████████| 3489/3489 [01:17<00:00, 44.74it/s]\n",
      "100%|██████████| 3489/3489 [00:58<00:00, 59.34it/s]\n",
      "100%|██████████| 3489/3489 [01:20<00:00, 43.40it/s]\n",
      "100%|██████████| 3489/3489 [01:07<00:00, 51.50it/s]\n",
      "Computing reprojection error....: 100%|██████████| 1650/1650 [00:09<00:00, 174.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error across views: 1.661813532405055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing reprojection error....:   6%|▌         | 96/1650 [00:00<00:09, 166.61it/s]/home/alexchan/Documents/3D-SOCS/3DTracking/Utils/BundleAdjustmentTool.py:188: RuntimeWarning: overflow encountered in square\n",
      "  EucError = np.sqrt(PointDiffs[:,0]**2 + PointDiffs[:,1]**2)\n",
      "Computing reprojection error....: 100%|██████████| 1650/1650 [00:15<00:00, 106.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error across views: 330.2637215079684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing reprojection error....: 100%|██████████| 1650/1650 [00:14<00:00, 110.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error across views: 2.890942341954614e+23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing reprojection error....: 100%|██████████| 1650/1650 [00:12<00:00, 127.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error across views: 60.87789823152159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing reprojection error....: 100%|██████████| 1650/1650 [00:12<00:00, 133.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error across views: 65602.1585496337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing reprojection error....: 100%|██████████| 1650/1650 [00:12<00:00, 130.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error across views: 75.062853170711\n",
      "Camera mill1 is best!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing reprojection error....: 100%|██████████| 1650/1650 [00:09<00:00, 172.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error across views: 1.661813532405055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.661813532405055"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next is extrinsics calibration\n",
    "ExtrinsicCalibration.AutoExtrinsicCalibrator(\n",
    "    VideoPaths, DataDir, None, CalibBoardDict, CamNames, None, PrimaryCamera=0, Undistort=True\n",
    ")\n",
    "\n",
    "# Get reprojection errors for extrinsic calibration\n",
    "ExtrinsicCalibration.GetReprojectErrors(CamNames, DataDir, VideoName=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412adc01-656b-49fb-9ea7-cddc3ef10bb7",
   "metadata": {},
   "source": [
    "The extrinsic calibration runs automatically, and tries to find the relative position between each camera. The resulting parameters are a rotation (R) and translation (T) parameter for each camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0395d9dc-f05f-4474-900a-9cf1aee156b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation Matrix\n",
      "[[ 0.9068442  -0.39809989  0.13838381]\n",
      " [ 0.08593723  0.49609628  0.86400421]\n",
      " [-0.41261167 -0.77162488  0.48409364]]\n",
      "Translation Matrix\n",
      "[[-253.66514532]\n",
      " [-682.21246857]\n",
      " [ 229.91136269]]\n"
     ]
    }
   ],
   "source": [
    "#Load example from camera 2:\n",
    "R, T = pickle.load(open(os.path.join(DataDir, \"mill2_Initial_Extrinsics.p\"), \"rb\"))\n",
    "            \n",
    "##Rotation matrix\n",
    "print(\"Rotation Matrix\")\n",
    "print(R)\n",
    "\n",
    "##Translation matrix\n",
    "print(\"Translation Matrix\")\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b5fdc",
   "metadata": {},
   "source": [
    "## 3D Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b982afce",
   "metadata": {},
   "source": [
    "Now that the calibration is done, the parameters are automatically saved in the data folder, and you can now do 3D tracking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d655c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexchan/anaconda3/envs/tf/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.3.3...\n",
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
     ]
    }
   ],
   "source": [
    "## Import stuff\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "sys.path.append(\"../Repositories/DeepLabCut-live\")\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from dlclive import DLCLive, Processor\n",
    "import deeplabcut as dlc\n",
    "from Utils import Track3DFunctions as T3D\n",
    "\n",
    "from natsort import natsorted\n",
    "\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2585c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have to define a list of keypoints, and the order it is outputted by the model\n",
    "INFERENCE_BODYPARTS = [\n",
    "    \"hd_bill_tip\",\n",
    "    \"hd_bill_base\",\n",
    "    \"hd_cheek_left\",\n",
    "    \"hd_cheek_right\",\n",
    "    \"hd_head_back\",\n",
    "    \"bd_tail_base\",\n",
    "    \"bd_bar_left_front\",\n",
    "    \"bd_bar_left_back\",\n",
    "    \"bd_bar_right_back\",\n",
    "    \"bd_bar_right_front\",\n",
    "    \"hd_eye_right\",\n",
    "    \"hd_eye_left\",\n",
    "    \"bd_shoulder_right\",\n",
    "    \"bd_tail_tip\",\n",
    "    \"hd_neck_right\",\n",
    "    \"bd_shoulder_left\",\n",
    "    \"hd_neck_left\",\n",
    "    \"bd_keel_top\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3b8cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define more inputs\n",
    "\n",
    "###Base video directory\n",
    "InputDir = \"../SampleDataset/3DTracking/bird_3B00186CDA_trial_32_time_2023-11-05 10_30_28.117871/\"\n",
    "\n",
    "###Where the calibration data is\n",
    "DataDir =  \"../SampleDataset/Calibration/data\"\n",
    "\n",
    "##Define data directory to store tracking info, and where the videos are\n",
    "SubDataDir = os.path.join(InputDir, \"data\")\n",
    "VideoDir = os.path.join(InputDir, \"videos\")\n",
    "\n",
    "#Camera names\n",
    "CamNames = [\"mill1\", \"mill2\", \"mill3\", \"mill4\", \"mill5\", \"mill6\"]\n",
    "\n",
    "##Path to the trained YOLO and DLC models\n",
    "YOLOPath = \"../SampleDataset/Weights/GretiBlutiYOLO.pt\"\n",
    "ExportModelPath = \"../SampleDataset/Weights/Greti_DLC\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7401b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running YOLO:: 100%|██████████| 138/138 [01:13<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "## First run YOLO for object detection!\n",
    "VideoFiles = natsorted([os.path.join(VideoDir, subfile) for subfile in os.listdir(VideoDir) if subfile.endswith(\".mp4\")])\n",
    "\n",
    "\n",
    "OutbboxDict, BBoxConfDict = T3D.RunYOLOTrack(YOLOPath, VideoFiles, DataDir, CamNames, startFrame=0, TotalFrames=-1, ScaleBBox=1, \n",
    "                                                FrameDiffs=[0]*len(CamNames), Objects=[\"greti\", \"bluti\"], Extrinsic=\"Initial\", undistort=True, YOLOThreshold=0.7)\n",
    "\n",
    "##Saving the data\n",
    "pickle.dump(OutbboxDict, open(os.path.join(SubDataDir, \"OutbboxDict.p\"), \"wb\"))\n",
    "pickle.dump(BBoxConfDict, open(os.path.join(SubDataDir, \"BBoxConfDict.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e7782",
   "metadata": {},
   "source": [
    "The output of the YOLO function are 2 dictionaries, one stores the bounding box outputs, the other stores the confidences.\n",
    "\n",
    "The dictionary is made of nested dictionaries structured as follows:\n",
    "```\n",
    "{\"frame\": {\n",
    "    \"CamName\":{\n",
    "    \"TrackingID\":[x1,y1,x2,y2]\n",
    "    ....\n",
    "}}}\n",
    "```\n",
    "First level is the frame number, then camera names, then individual bounding boxes for each tracking ID. The bounding box confidences are stored in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00e19a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mill1': {'mill1-greti-1.0': [1295.8067626953125,\n",
       "   208.7423095703125,\n",
       "   1493.1690673828125,\n",
       "   648.7586669921875],\n",
       "  'mill1-greti-2.0': [1661.8609008789062,\n",
       "   482.964599609375,\n",
       "   1919.7001342773438,\n",
       "   1080.0]},\n",
       " 'mill2': {'mill2-greti-1.0': [502.1390380859375,\n",
       "   257.21807861328125,\n",
       "   681.802001953125,\n",
       "   499.021484375]},\n",
       " 'mill3': {'mill3-greti-2.0': [376.87371826171875,\n",
       "   94.2266845703125,\n",
       "   560.5238647460938,\n",
       "   303.81610107421875]},\n",
       " 'mill4': {'mill4-greti-1.0': [80.33905029296875,\n",
       "   301.81353759765625,\n",
       "   607.6127319335938,\n",
       "   553.2652587890625]},\n",
       " 'mill5': {},\n",
       " 'mill6': {'mill6-greti-1.0': [623.7623291015625,\n",
       "   359.3936767578125,\n",
       "   828.4041748046875,\n",
       "   547.9381103515625]}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Example for frame 0\n",
    "OutbboxDict = pickle.load(open(os.path.join(SubDataDir, \"OutbboxDict.p\"), \"rb\"))\n",
    "OutbboxDict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b223b",
   "metadata": {},
   "source": [
    "Next, we run the DeepLabCut model to get 2D postures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "243f9f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running DLC:: 100%|██████████| 138/138 [01:13<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "##Prepare deeplabcut inference model\n",
    "dlc_proc = Processor()\n",
    "dlc_liveObj = DLCLive(ExportModelPath, processor=dlc_proc)\n",
    "\n",
    "\n",
    "OutbboxDict = pickle.load(open(os.path.join(SubDataDir, \"OutbboxDict.p\"), \"rb\"))\n",
    "BBoxConfDict = pickle.load(open(os.path.join(SubDataDir, \"BBoxConfDict.p\"), \"rb\"))\n",
    "\n",
    "# Run DeepLabCut to track body parts in the detected objects\n",
    "Out2DDict = T3D.RunDLC(dlc_liveObj, OutbboxDict, BBoxConfDict, DataDir, \n",
    "                        VideoFiles, CamNames, CropSize=(320,320), INFERENCE_BODYPARTS=INFERENCE_BODYPARTS,\n",
    "                        startFrame=0, TotalFrames=-1, FrameDiffs=[0]*len(CamNames), Extrinsic=\"Initial\",\n",
    "                        DLCConfidenceThresh=0.3, YOLOThreshold=0.5,\n",
    "                        undistort=True)\n",
    "\n",
    "# Save DeepLabCut output\n",
    "pickle.dump(Out2DDict, open(os.path.join(SubDataDir, \"Out2DDict.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc62a1",
   "metadata": {},
   "source": [
    "The output of the 2D keypoint detection step is similar to YOLO, also nested dictionaries.\n",
    "\n",
    "Here is the structure:\n",
    "```\n",
    "{\"frame\": {\n",
    "    \"CamName\":{\n",
    "    \"TrackingID\":{\n",
    "        \"ID_KeypointName\":[x,y]\n",
    "    }\n",
    "    ....\n",
    "}}}\n",
    "```\n",
    "\n",
    "First level is frame number, then camera names, then tracking ID, then individual keypoint names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8adf5f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mill1': {'mill1-greti-1.0': {'mill1-greti-1.0_hd_head_back': [1337.6382026672363,\n",
       "    571.0718994140625],\n",
       "   'mill1-greti-1.0_bd_bar_left_back': [1459.9082336425781,\n",
       "    466.63531494140625],\n",
       "   'mill1-greti-1.0_bd_bar_right_back': [1393.0919799804688,\n",
       "    475.8860778808594],\n",
       "   'mill1-greti-1.0_bd_bar_right_front': [1357.7661628723145,\n",
       "    497.9913024902344]}},\n",
       " 'mill2': {'mill2-greti-1.0': {'mill2-greti-1.0_hd_bill_base': [520.6679267883301,\n",
       "    479.22900390625],\n",
       "   'mill2-greti-1.0_hd_cheek_left': [569.3762283325195, 447.8141174316406],\n",
       "   'mill2-greti-1.0_hd_head_back': [545.3655242919922, 419.7465362548828],\n",
       "   'mill2-greti-1.0_bd_tail_base': [628.4700775146484, 307.0471954345703],\n",
       "   'mill2-greti-1.0_bd_bar_left_front': [632.3062591552734,\n",
       "    386.95167541503906],\n",
       "   'mill2-greti-1.0_bd_bar_left_back': [623.7471771240234, 351.99369049072266],\n",
       "   'mill2-greti-1.0_hd_eye_left': [544.5481491088867, 469.30853271484375]}},\n",
       " 'mill3': {'mill3-greti-2.0': {'mill3-greti-2.0_bd_tail_base': [446.3436737060547,\n",
       "    136.3617172241211],\n",
       "   'mill3-greti-2.0_bd_bar_right_back': [477.0027084350586, 162.6028594970703],\n",
       "   'mill3-greti-2.0_bd_bar_right_front': [470.1195373535156,\n",
       "    190.31175231933594],\n",
       "   'mill3-greti-2.0_bd_shoulder_right': [497.7020797729492, 227.224853515625],\n",
       "   'mill3-greti-2.0_hd_neck_right': [494.2385940551758, 265.2358856201172]}},\n",
       " 'mill4': {'mill4-greti-1.0': {'mill4-greti-1.0_hd_cheek_right': [513.7705688476562,\n",
       "    440.4673156738281],\n",
       "   'mill4-greti-1.0_bd_tail_base': [281.7896423339844, 353.2179946899414],\n",
       "   'mill4-greti-1.0_bd_bar_right_back': [401.6734619140625,\n",
       "    336.60619354248047],\n",
       "   'mill4-greti-1.0_bd_bar_right_front': [421.1767578125, 385.1035919189453],\n",
       "   'mill4-greti-1.0_hd_eye_right': [530.6011047363281, 483.19342041015625],\n",
       "   'mill4-greti-1.0_bd_shoulder_right': [503.50079345703125,\n",
       "    416.13724517822266],\n",
       "   'mill4-greti-1.0_hd_neck_right': [479.3204345703125, 467.7480163574219]}},\n",
       " 'mill5': {},\n",
       " 'mill6': {'mill6-greti-1.0': {'mill6-greti-1.0_bd_tail_base': [732.7891235351562,\n",
       "    454.34214782714844],\n",
       "   'mill6-greti-1.0_bd_bar_left_front': [643.591911315918, 427.5178909301758],\n",
       "   'mill6-greti-1.0_bd_bar_left_back': [668.3697090148926,\n",
       "    408.80006408691406]}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example for frame 0\n",
    "Out2DDict = pickle.load(open(os.path.join(SubDataDir, \"Out2DDict.p\"), \"rb\"))\n",
    "Out2DDict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ca27f",
   "metadata": {},
   "source": [
    "After getting 2D keypoint estimates, we have to triangulate them into 3D coordinates. Before we do this, we also quickly run a 2D rolling average filter to make the detection less jittery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fe3b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Doing Matching...: 100%|██████████| 137/137 [00:02<00:00, 48.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mill4-greti-94.0', 'mill5-greti-96.0', 'mill2-greti-86.0', 'mill1-greti-25.0', 'mill5-greti-22.0', 'mill2-greti-26.0', 'mill4-greti-20.0', 'mill6-greti-23.0', 'mill1-greti-2.0', 'mill4-bluti-20.0', 'mill3-greti-28.0'}, {'mill2-greti-1.0', 'mill1-greti-1.0', 'mill6-greti-1.0', 'mill4-greti-1.0', 'mill5-greti-4.0', 'mill3-greti-2.0'}, {'mill3-bluti-2.0', 'mill1-bluti-64.0', 'mill6-greti-53.0', 'mill2-bluti-60.0', 'mill5-bluti-4.0', 'mill6-bluti-53.0', 'mill2-greti-100.0', 'mill4-bluti-1.0'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Triangulating...: 100%|██████████| 137/137 [00:02<00:00, 55.10it/s]\n"
     ]
    }
   ],
   "source": [
    "##Load outputs from inference\n",
    "Out2DDict = pickle.load(open(os.path.join(SubDataDir, \"Out2DDict.p\"), \"rb\"))\n",
    "BBoxConfDict = pickle.load(open(os.path.join(SubDataDir, \"BBoxConfDict.p\"), \"rb\"))\n",
    "\n",
    "Filtered2DDict = T3D.Filter2D(Out2DDict, CamNames, INFERENCE_BODYPARTS, WindowSize=3)\n",
    "pickle.dump(Filtered2DDict, open(os.path.join(SubDataDir, \"Filtered2DDict.p\"), \"wb\"))\n",
    "\n",
    "\n",
    "##Do matching and triangulation\n",
    "Out3DDict, Out2DDict = T3D.TriangulateFromPointsMulti(Filtered2DDict, BBoxConfDict, DataDir, \n",
    "                                                        CamNames, INFERENCE_BODYPARTS, Extrinsic=\"Initial\",\n",
    "                                                        TotalFrames=-1, YOLOThreshold=0.5, CombinationThreshold=0.7,\n",
    "                                                        DminThresh=50)\n",
    "\n",
    "pickle.dump(Out3DDict, open(os.path.join(SubDataDir, \"Out3DDict.p\"), \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231b13d",
   "metadata": {},
   "source": [
    "3D keypoint data is also stored in a similar way, in nested dictionaries.\n",
    "\n",
    "Here is the structure:\n",
    "```\n",
    "{\"frame\": {\n",
    "        \"ID_KeypointName\":[x,y,z]\n",
    "    ....\n",
    "}}\n",
    "```\n",
    "\n",
    "First level is frame number, then keypoints names, with the first string before the first underscore (\"_\") as the ID of the bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61c08530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_bd_bar_left_back': array([      147.2,     -30.446,      770.29]),\n",
       " '1_bd_bar_left_front': array([     152.43,     -22.492,      782.09]),\n",
       " '1_bd_bar_right_back': array([     125.27,      -28.64,      763.35]),\n",
       " '1_bd_bar_right_front': array([     116.83,     -23.613,      773.84]),\n",
       " '1_bd_tail_base': array([        134,     -59.356,      768.34])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Example for frame 0\n",
    "Out3DDict = pickle.load(open(os.path.join(SubDataDir, \"Out3DDict.p\"), \"rb\"))\n",
    "Out3DDict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef485a1",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312cc58a",
   "metadata": {},
   "source": [
    "Finally is post processing, there are multiple steps involved, and is described in detail in the manuscript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67ec2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import stuff\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"Utils/\")\n",
    "import PostProcessFunctions as PP\n",
    "\n",
    "#Define stuff\n",
    "InputDir = \"../SampleDataset/3DTracking/bird_3B00186CDA_trial_32_time_2023-11-05 10_30_28.117871/\"\n",
    "DataDir = \"../SampleDataset/Calibration/data\"\n",
    "SubDataDir = os.path.join(InputDir, \"data\")\n",
    "\n",
    "# Define camera names and their respective image sizes\n",
    "CamNames = [\"mill1\", \"mill2\", \"mill3\", \"mill4\", \"mill5\", \"mill6\"]\n",
    "imsizeDict = {\n",
    "    \"mill1\": (1920, 1080), \"mill2\": (1280, 720), \"mill3\": (1280, 720),\n",
    "    \"mill4\": (1280, 720), \"mill5\": (1280, 720), \"mill6\": (1280, 720)\n",
    "}\n",
    "\n",
    "# Load inference files\n",
    "Out3DDict = pickle.load(open(os.path.join(SubDataDir, \"Out3DDict.p\"), \"rb\"))\n",
    "Out2DDict = pickle.load(open(os.path.join(SubDataDir, \"Filtered2DDict.p\"), \"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fc46502",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First filter: reproject points and filter if the reprojection is outside of the screen\n",
    "FilterReprojectOut = PP.FilterbyReproject(Out3DDict, DataDir, CamNames, imsizeDict, Ratio=0.5, Extrinsic=\"Initial\")\n",
    "\n",
    "## Second filter: filter by distance to mean head and body\n",
    "\n",
    "# Define head and body points for filtering\n",
    "HeadPoints = [\"hd_eye_right\", \"hd_eye_left\", \"hd_cheek_left\", \"hd_cheek_right\", \"hd_bill_tip\", \"hd_bill_base\", \"hd_neck_left\", \"hd_neck_right\"]\n",
    "BodyPoints = [\"bd_bar_left_front\", \"bd_bar_left_back\", \"bd_bar_right_front\", \"bd_bar_right_back\", \"bd_tail_base\", \"bd_keel_top\", \"bd_tail_tip\", \"bd_shoulder_left\", \"bd_shoulder_right\"]\n",
    "\n",
    "# Filter points by head and body\n",
    "Filtered_3DDict = PP.FilterPoints(FilterReprojectOut, HeadPoints)\n",
    "Filtered_3DDict = PP.FilterPoints(Filtered_3DDict, BodyPoints)\n",
    "\n",
    "## Interpolate data\n",
    "Linear3DDict = PP.InterpolateData(Filtered_3DDict, Type=\"pchip\", interval=1, show=False)\n",
    "pickle.dump(Linear3DDict, open(os.path.join(SubDataDir, \"MultiFiltered_Out3DDict.p\"), \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e4c05e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done!\n"
     ]
    }
   ],
   "source": [
    "## Next, we use the median species skull and \"slot in\" the head based on the rotation of head keypoints\n",
    "Filtered_3DDict = pickle.load(open(os.path.join(SubDataDir, \"MultiFiltered_Out3DDict.p\"), \"rb\"))\n",
    "\n",
    "##First save the data, as this function does this per individual\n",
    "pickle.dump(Linear3DDict, open(os.path.join(SubDataDir, \"MultiObjectFiltered_Out3DDict.p\"), \"wb\"))\n",
    "\n",
    "##Find all unique IDs\n",
    "Out3DDictKeys = [list(subDict.keys()) for subDict in Linear3DDict.values()]\n",
    "Out3DDictKeysFlat = []\n",
    "[Out3DDictKeysFlat.extend(x) for x in Out3DDictKeys]\n",
    "UnqIDs = sorted(list(set([x.split(\"_\")[0] for x in list(set(Out3DDictKeysFlat))])))\n",
    "SpeciesObjectDict = pickle.load(open(os.path.join(InputDir, \"MedianSpeciesObjects.p\"), \"rb\"))  # Median skeleton for species!\n",
    "\n",
    "# Filter by object for each individual ID\n",
    "for IndID in UnqIDs:\n",
    "    Linear3DDict = pickle.load(open(os.path.join(SubDataDir, \"MultiObjectFiltered_Out3DDict.p\"), \"rb\"))\n",
    "\n",
    "    HeadFilteredDictRotation = PP.FilterbyObject(Linear3DDict, SpeciesObjectDict[\"GRETI\"][\"hd\"], HeadPoints, IndID, ObjFilterThresh=50)\n",
    "    BodyFilteredDictRotation = PP.FilterbyObject(HeadFilteredDictRotation, SpeciesObjectDict[\"GRETI\"][\"bd\"], BodyPoints, IndID, ObjFilterThresh=50)\n",
    "\n",
    "    # Save the filtered data\n",
    "    pickle.dump(BodyFilteredDictRotation, open(os.path.join(SubDataDir, \"MultiObjectFiltered_Out3DDict.p\"), \"wb\"))\n",
    "\n",
    "# Final interpolation of the filtered data\n",
    "FinalOut3DDict = PP.InterpolateData(BodyFilteredDictRotation, Type=\"pchip\", interval=1, show=False)\n",
    "\n",
    "# Save the final processed data\n",
    "pickle.dump(FinalOut3DDict, open(os.path.join(SubDataDir, \"Out3DDictProcessed.p\"), \"wb\"))\n",
    "print(\"Processing done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962f7bb",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df8ed31",
   "metadata": {},
   "source": [
    "Finally, you can visualize the data with overlayed visual fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb27c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and define stuff\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "sys.path.append(\"Utils\")\n",
    "from ReprojectVisualFields import Visualize3D_VS\n",
    "\n",
    "# Dictionary to map body parts to their respective colors for visualization\n",
    "ColourDictionary = {\n",
    "    \"hd_bill_tip\": (180, 95, 26),\n",
    "    \"hd_bill_base\":(92, 228, 248),\n",
    "    \"hd_eye_left\" :(137, 227, 87),\n",
    "    \"hd_eye_right\":(59, 51, 237),\n",
    "    \"hd_cheek_left\":(241, 193, 153),\n",
    "    \"hd_cheek_right\":(0, 120, 255),\n",
    "    \"hd_neck_left\": (156, 61, 129),\n",
    "    \"hd_neck_right\":(17, 194, 245),\n",
    "    \"hd_head_back\":(90, 131, 181),\n",
    "    \"bd_bar_left_front\":(216, 113, 28),\n",
    "    \"bd_bar_left_back\":(126, 194, 46),\n",
    "    \"bd_bar_right_front\":(0, 120, 255),\n",
    "    \"bd_bar_right_back\":(81, 97, 246),\n",
    "    \"bd_tail_base\":  (68, 106, 152)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b96ec72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define parameters\n",
    "# Input directory containing the dataset\n",
    "InputDir = \"../SampleDataset/3DTracking/bird_3B00186CDA_trial_32_time_2023-11-05 10_30_28.117871/\"\n",
    "# Directory containing calibration data\n",
    "DataDir = \"../SampleDataset/Calibration/data\"\n",
    "# Subdirectory containing the processed data\n",
    "SubDataDir = os.path.join(InputDir,\"data\")\n",
    "# Directory containing the video files\n",
    "VideoDir = os.path.join(InputDir,\"videos\")\n",
    "\n",
    "# List of camera names\n",
    "CamNames = [\"mill1\",\"mill2\",\"mill3\",\"mill4\",\"mill5\",\"mill6\"]\n",
    "\n",
    "# Get sorted list of video files\n",
    "VideoFiles = natsorted([os.path.join(VideoDir,subfile) for subfile in os.listdir(VideoDir) if subfile.endswith(\".mp4\")])\n",
    "# Load the processed 3D data dictionary\n",
    "Out3DDict = pickle.load(open(os.path.join(SubDataDir,\"Out3DDictProcessed.p\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d78d07b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [00:02<00:00, 49.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the 3D tracking data\n",
    "Visualize3D_VS(Out3DDict, DataDir, VideoFiles, CamNames, FrameDiffs=[0]*len(CamNames),\n",
    "                startFrame=0, TotalFrames=-1, VisualizeIndex=2, ColourDictionary=ColourDictionary,\n",
    "                Extrinsic=\"Initial\", show=False, save=True, VSAngle=60, ElvAngle=0, Magnitude=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6d8f4",
   "metadata": {},
   "source": [
    "This saves an output video in the current directory, you can take a look at the tracking results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
